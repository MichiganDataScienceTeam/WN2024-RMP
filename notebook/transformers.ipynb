{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "A type of model in NLP that forms the basis of many state-of-art LLM today such as ChatGPT. They are originally focused in NLP tasks but was later expanded to areas like Computer Vision, Audio Processing and many more. In our case of sentiment analysis, we are more concerned with the NLP side of these models. \n",
    "\n",
    "Several popular transformer models that are commonly used for NLP tasks are\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is designed to understand the context of words in a sentence by looking at them in both directions (left-to-right and right-to-left). It's often used as a base for many NLP tasks.\n",
    "\n",
    "- **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: This is a variant of BERT developed by Facebook. It modifies BERT's training approach for improved performance.\n",
    "\n",
    "-  **DistilBERT**: This is a smaller, faster, and lighter version of BERT developed by Hugging Face. It retains 95% of BERT's performance while being 60% smaller and 60% faster.\n",
    "\n",
    "- **GPT (Generative Pretrained Transformer)** and **GPT-2**: Developed by OpenAI, these models are designed for tasks that require generating text, but they can also be fine-tuned for text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face is a company and a platform that focuses on natural language processing (NLP) and provides tools, libraries, and resources to facilitate NLP research, development, and applications. \n",
    "\n",
    "On Hugging Face, you can find a set of pretrained Transformer models. \n",
    "\n",
    "There are two main ways to use them:\n",
    "1. Install the models to your enviornment and use them with the Pipline function. Which is part of the Hugging Face library thatencapsulates the complex process of applying a transformer model into simple function calls. It help apply varies pre-trained transformer models to different tasks.\n",
    "\n",
    "2. Use the Hosted Inference API, which allows users to perform inference (make predictions) using Hugging Face models remotely through web API calls. It avoid the overhead of managing model infrastructure locally. \n",
    "\n",
    "We will be using the second option.\n",
    "\n",
    "First, get an API key if you haven't done so: https://huggingface.co/docs/api-inference/index. Then follow the below steps:\n",
    "\n",
    "```\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "Create a .env file in your project's directory and add your API key:\n",
    "```\n",
    "API_KEY=your_api_key\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you please let us know more details about your iphone so we can get some more information?\"\\n\\n\"Just say yes, I\\'m open to any suggestions.\"\\n\\nTia looks at me worried.\\n\\n\"That\\'s fine'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an example taken from the Hugging Face guide. If you are able to run this, you successfully setted up for calling the Inference API\n",
    "# ENDPOINT Template: https://api-inference.huggingface.co/models/<MODEL_ID>. Change the model ID for different models\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "my_headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=my_headers, json=payload)\n",
    "    return response.json()\n",
    "data = query(\"Can you please let us know more details about your \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using and query different models, let's refactor the code to a higher order function so that we do not have to redefine a new query function for every model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_query_runner(endpoint: str):\n",
    "    def run_query(query: str):\n",
    "        response = requests.post(endpoint, headers = my_headers, json=query)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            raise Exception(f\"Query failed and returned status code {response.status_code}. {response.json()}\")\n",
    "    return run_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let' load up our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RATING_PATH = \"../data/clean_ratings.csv\"\n",
    "PROF_PATH = \"../data/clean_prof_info.csv\"\n",
    "\n",
    "rating = pd.read_csv(RATING_PATH)\n",
    "prof = pd.read_csv(PROF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the DistilBERT base uncased finetuned SST-2\n",
    "You can read more descriptions here: https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english \n",
    "\n",
    "This model does a polarity-based sentiment analysis, which is similar to what we seen in NLTK VADAR, it outputs only a positivity and negativity score/percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'POSITIVE', 'score': 0.9998738765716553}, {'label': 'NEGATIVE', 'score': 0.0001261125726159662}]]\n"
     ]
    }
   ],
   "source": [
    "distilBert_sst2_endpoint= \"https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "s = \"I like you. I love you\"\n",
    "run_query_on_distilBert = api_query_runner(distilBert_sst2_endpoint)\n",
    "data = run_query_on_distilBert(s)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see output of {'error': 'Model distilbert/distilbert-base-uncased-finetuned-sst-2-english is currently loading', 'estimated_time': 20.0}. Just wait for a little and try again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run it for all EECS376 class reviews. It may take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'label': 'POSITIVE', 'score': 0.9994785189628601},\n",
       "   {'label': 'NEGATIVE', 'score': 0.0005215604905970395}]],\n",
       " [[{'label': 'NEGATIVE', 'score': 0.9988952875137329},\n",
       "   {'label': 'POSITIVE', 'score': 0.001104680704884231}]],\n",
       " [[{'label': 'POSITIVE', 'score': 0.9998089671134949},\n",
       "   {'label': 'NEGATIVE', 'score': 0.00019110905122943223}]],\n",
       " [[{'label': 'NEGATIVE', 'score': 0.9997746348381042},\n",
       "   {'label': 'POSITIVE', 'score': 0.00022532072034664452}]],\n",
       " [[{'label': 'POSITIVE', 'score': 0.9995610117912292},\n",
       "   {'label': 'NEGATIVE', 'score': 0.0004389724927023053}]]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eecs376_comments = rating[rating[\"class\"]==\"EECS376\"][\"comment\"]\n",
    "first_five = eecs376_comments[:5]\n",
    "sentiment = [run_query_on_distilBert(comment) for comment in first_five]\n",
    "sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
