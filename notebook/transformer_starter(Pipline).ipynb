{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers with Pipline\n",
    "\n",
    "A type of model in NLP that forms the basis of many state-of-art model such as BERT and GPT-2\n",
    "\n",
    "This is the starter file for downloading Transformers to local environment. However, it has limited information, as I stopped working on it when deciding to shift to using Web API for this stage of the project. For detailed tutorial, checkout office guide on [Pipline for Inference](https://huggingface.co/docs/transformers/en/pipeline_tutorial).\n",
    "\n",
    "We will come back to working with models locally in the future for finetuning and training stage of the project(potentially). So it is good to keep this file around\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Pipline](https://huggingface.co/docs/transformers/pipeline_tutorial) is a wrapper function in the Hugging Face's Transformers libarary that abstract away the complex process of preprocessing input data, loading the model, making a prediction, and process output data into a simple function call. Leaving us an interface that only requires a couple lines of code to make a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # This will take some time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important parameters to specify in the Hugging Face pipline are: \n",
    "\n",
    "Task: A string that specify the task to perform such as sentiment analysis, question-answering, summation, translation etc. \n",
    "\n",
    "Model: A model ID for a specific transformer model.\n",
    "\n",
    "If any of these are missing, they will be defaulted base on the input of the other parameter.\n",
    "\n",
    "For example, if Model is left blank, then the default model for the specified task will be selected. If the Task is left blank, then the default task for the specifed model will be selected.\n",
    "\n",
    "Note: Some model can do multiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9916695356369019},\n",
       " {'label': 'NEG', 'score': 0.9806600213050842}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring transformer with RMP comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting class sentiment and check out the actual comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9641988277435303},\n",
       " {'label': 'POS', 'score': 0.9928324222564697},\n",
       " {'label': 'POS', 'score': 0.9918490648269653},\n",
       " {'label': 'POS', 'score': 0.9914464950561523},\n",
       " {'label': 'POS', 'score': 0.9928606152534485}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = pd.read_csv(\"../data/clean_ratings.csv\")\n",
    "first_five = rating[\"comment\"][:5].tolist()\n",
    "# Take the first 5 random comment\n",
    "\n",
    "sentiment_pipeline(first_five)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fricke is the man. Entire class probably took five hours of studying for the entire semester. I think he may be retiring, but if not, take this class if you can fit it in.\n",
      "Tom Fricke is one of those professors you will never, ever forget. I found myself coming to every single lecture even though none of them were mandatory simply because he is quite possibly the most entertaining person on the planet. Tom cares so much about his students and I became quite good friends with him through office hours. 100/10 recommend\n",
      "Prof. Fricke is amazing. He is hilarious and tells great, interesting stories. Practice exams are the actual exams, but Fricke doesn't give the answers directly (he will if you ask him). If you have the option, TAKE THIS CLASS!\n",
      "Such an easy class. Exams were exactly like the practice exam given and I didn't have to show up to lecture. He's also a great guy and I went to office hours just to chat with him.\n",
      "Easiest class i have taken at UM. The exams took a majority of the class about 15-20 minutes if you looked at the practice exam he gave you!!! He's so interesting to listen to and you could tell he's extremely passionate about his work which made lectures so enjoyable and fun. I highly recommend taking this class!!!!\n"
     ]
    }
   ],
   "source": [
    "for comment in first_five: print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average positive score: 0.9592511071978023\n",
      "Average negative score: 0.8555095936570849\n"
     ]
    }
   ],
   "source": [
    "eecs280 = rating[rating[\"class\"]==\"EECS280\"][\"comment\"]\n",
    "eecs280 = eecs280[eecs280 != \"No Comments\"]\n",
    "results = sentiment_pipeline(eecs280.tolist())\n",
    "\n",
    "positive_scores = [result['score'] for result in results if result['label']=='POS']\n",
    "negative_scores = [result['score'] for result in results if result['label'] == 'NEG']\n",
    "\n",
    "average_positive_score = sum(positive_scores)/len(positive_scores) if positive_scores else 0\n",
    "average_negative_score = sum(negative_scores) / len(negative_scores) if negative_scores else 0 \n",
    "\n",
    "print(f'Average positive score: {average_positive_score}')\n",
    "print(f'Average negative score: {average_negative_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive comments are very positive, the negative comments are pretty negative, but not extremely negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average positivity using the comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class(regardless of professor) using summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the comments as one big string of text first. \n",
    "comment_batches = []\n",
    "\n",
    "# Summarization task have a cap for maximum input tokens, thus we put into batches of 30\n",
    "for comments in range(0, len(eecs280), 10):\n",
    "    batch = ''.join(eecs280[comments:comments+10])\n",
    "    comment_batches.append(batch)\n",
    "summarizer = pipeline(\"summarization\", model =\"sshleifer/distilbart-cnn-12-6\")\n",
    "summary = summarizer(comment_batches[0], max_length = 100, min_length = 50, do_sample=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" He knows OOP, if you pay attention he will teach you. Take advantage of office hours! Conveniently posts lecture notes online so that students can focus on the material and not worry about scribbling down every last bit of code. He's an akward, pacing computer dork.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Do not waist your money and your time by taking a class that she teaches . She speaks too fast and with a heavy accent, and goes through work way too quickly . Exams are killers - very unreasonable . She's actually nice to talk to if you make an effort to go to her office hours .\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try another example: \n",
    "summary = summarizer(comment_batches[1], max_length = 100, min_length = 50, do_sample=False )\n",
    "summary[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
