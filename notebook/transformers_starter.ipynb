{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "A type of model in NLP that forms the basis of many state-of-art LLM today such as ChatGPT. They are originally focused in NLP tasks but was later expanded to areas like Computer Vision, Audio Processing and many more. In our case of sentiment analysis, we are more concerned with the NLP side of these models. \n",
    "\n",
    "Several popular transformer models that are commonly used for NLP tasks are\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is designed to understand the context of words in a sentence by looking at them in both directions (left-to-right and right-to-left). It's often used as a base for many NLP tasks.\n",
    "\n",
    "- **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: This is a variant of BERT developed by Facebook. It modifies BERT's training approach for improved performance.\n",
    "\n",
    "-  **DistilBERT**: This is a smaller, faster, and lighter version of BERT developed by Hugging Face. It retains 95% of BERT's performance while being about 60% smaller and 60% faster.\n",
    "\n",
    "- **GPT (Generative Pretrained Transformer)** and **GPT-2**: Developed by OpenAI, these models are designed for tasks that require generating text, but they can also be fine-tuned for text classification tasks.\n",
    "\n",
    "(Optional, come back to this later) More information on how Transformers work under the hood: https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face is a company and a platform that focuses on natural language processing (NLP) and provides tools, libraries, and resources to facilitate NLP research, development, and applications. \n",
    "\n",
    "On Hugging Face, you can find a set of pretrained Transformer models. \n",
    "\n",
    "There are two main ways to use them:\n",
    "1. Install the models to your enviornment(via the Transformers Python library) and use them with the Pipline function. Which is part of the Hugging Face library that encapsulates the complex process of applying a transformer model into simple function calls. We can use the Pipline function for varies pre-trained transformer models to do different tasks.\n",
    "\n",
    "2. Use the Hosted Inference API, which allows users to perform inference (make predictions) using Hugging Face models remotely through web API calls. It avoid the overhead of managing model infrastructure locally. \n",
    "\n",
    "We will be using the second option, but I also wrote a quick starter file notebook for the first option [here](./transformer_starter(Pipline).ipynb)\n",
    "\n",
    "#### Getting started with the second option*: \n",
    "\n",
    "First, get an API key if you haven't done so: https://huggingface.co/docs/api-inference/index. Then follow the below steps:\n",
    "\n",
    "```\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "Create a .env file in your project's directory and add your API key:\n",
    "```\n",
    "API_KEY=\"your_api_key\"\n",
    "```\n",
    "\n",
    "Then, install the requests library:\n",
    "\n",
    "```\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "If you are using Google Colab, you could store the keys Collab's \"secret\" and change the code below \\(about three lines) correspondingly. Colab will provide you necessary steps. Just use the navbar on the left side of the screen.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*5wEevNCOf80GTHwptPTB4g.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you please let us know more details about your !!\\n\\nThankyou!\\n\\n\\nThis review may not be completed.\\n\\nDownload\\n\\nThe following product may be unavailable, or at the least that is not listed below in our database'}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an example taken from the Hugging Face guide. If you are able to run this, you are successfully setted up for calling the Inference API\n",
    "import requests\n",
    "from dotenv import load_dotenv # Change if you are using Collab\n",
    "import os\n",
    "\n",
    "# Take environment variables from .env.\n",
    "load_dotenv()  # Change if you are using Collab \n",
    "api_key = os.getenv(\"API_KEY\") # Change if you are using Collab\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "my_headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=my_headers, json=payload)\n",
    "    return response.json()\n",
    "data = query(\"Can you please let us know more details about your \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using and query different models, let's refactor the code into a higher order function so that we do not have to redefine a new query function for every model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_query_runner(endpoint: str):\n",
    "    def run_query(query: str):\n",
    "        response = requests.post(endpoint, headers = my_headers, json=query)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            raise Exception(f\"Query failed and returned status code {response.status_code}. {response.json()}\")\n",
    "    return run_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let' load up our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RATING_PATH = \"../data/clean_ratings.csv\"\n",
    "PROF_PATH = \"../data/clean_prof_info.csv\"\n",
    "\n",
    "rating = pd.read_csv(RATING_PATH)\n",
    "prof = pd.read_csv(PROF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the DistilBERT base uncased finetuned SST-2\n",
    "You can read more descriptions here: https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
    "\n",
    "Models usually the naming template of (basemodel) base (dataset trained-on).\n",
    "And if applicable, dataset fintuned on at the end\n",
    "\n",
    "This model does a polarity-based sentiment analysis, which is similar to what we seen in NLTK VADAR, it outputs only a positivity and negativity score/percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998756647109985}, {'label': 'NEGATIVE', 'score': 0.00012428968329913914}]\n"
     ]
    }
   ],
   "source": [
    "distilBert_sst2_endpoint= \"https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "s = [\"I like you.\"]\n",
    "run_query_on_distilBert = api_query_runner(distilBert_sst2_endpoint)\n",
    "data = run_query_on_distilBert(s)\n",
    "# Returned data is a list of list. Since we run the query with only one input string, there is only one item in the sublist\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see output of {'error': 'Model distilbert/distilbert-base-uncased-finetuned-sst-2-english is currently loading', 'estimated_time': 20.0}. It means the model is being prepared on the server. Just wait for a little and try again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run it for five EECS376 class reviews. \n",
    "The code below fetch five comments and comment ID from the rating dataset that are of class EECS376 and query the comments individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'ID': 14700},\n",
       "  {'label': 'POSITIVE', 'score': 0.9994785189628601},\n",
       "  {'label': 'NEGATIVE', 'score': 0.0005215604905970395}],\n",
       " [{'ID': 14701},\n",
       "  {'label': 'NEGATIVE', 'score': 0.9988952875137329},\n",
       "  {'label': 'POSITIVE', 'score': 0.001104680704884231}]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eecs376 = rating[rating[\"class\"]==\"EECS376\"]\n",
    "first_five_comments= eecs376[\"comment\"][:5]\n",
    "\n",
    "# Returned data is a list of list. Since we run the query with only one input string, there is only one item in the sublist, \n",
    "# hence we use index: [0]\n",
    "sentiment = [[{\"ID\": index}] + run_query_on_distilBert(comment)[0] for index, comment in first_five_comments.items()]\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our code send an API call for each comment, this is inefficient as each call involves a round trip to the server. Furthermore, we may reach a rate limit if we send a lot of requests at one time. Though this is not likely to happen with the amount of queries we are making. Anyway, a better way to call the API is simply send the comments in batches. In our case, we will just query with all of the comments in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'ID': 14700},\n",
       "  {'label': 'POSITIVE', 'score': 0.9994785189628601},\n",
       "  {'label': 'NEGATIVE', 'score': 0.0005215604905970395}],\n",
       " [{'ID': 14701},\n",
       "  {'label': 'NEGATIVE', 'score': 0.9988952875137329},\n",
       "  {'label': 'POSITIVE', 'score': 0.001104680704884231}]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run_query_on_distilBert(first_five_comments.tolist())\n",
    "# Zip the cooresponding ID for each comment back together\n",
    "for entry, index in zip(result, first_five_comments.index):\n",
    "    entry.insert(0, {\"ID\": index})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, doing this really provides limited information. Maybe instead we can determine a threshold or convert the positive and negative percentage metrics into a single label indicating whether the review is positive or negative. Thus we can aggregate the positive and negative reviews and see how many of the students liked the course and how many didn't. \n",
    "\n",
    "Ex: For a course, make API query with all of the reviews on that course. Then, for a review that have a higher \"POSITIVE\" score than \"NEGATIVE\" score, we label that review POSITIVE and vice versa. Now, we have a count of total positive reviews and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Explore this model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Roberta base go emotions\n",
    "You can read more descriptions here: https://huggingface.co/SamLowe/roberta-base-go_emotions\n",
    "\n",
    "A positive and negative overall label does not provide us much useful information beyond the general \"likeness\" of the course. We can use models that output an emotion prediction for more insights on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'joy', 'score': 0.7484319806098938},\n",
       "  {'label': 'relief', 'score': 0.0860075056552887},\n",
       "  {'label': 'approval', 'score': 0.07444580644369125},\n",
       "  {'label': 'neutral', 'score': 0.06166863813996315},\n",
       "  {'label': 'gratitude', 'score': 0.038480933755636215},\n",
       "  {'label': 'realization', 'score': 0.017022809013724327},\n",
       "  {'label': 'admiration', 'score': 0.016435207799077034},\n",
       "  {'label': 'caring', 'score': 0.016028305515646935},\n",
       "  {'label': 'disapproval', 'score': 0.011067488230764866},\n",
       "  {'label': 'annoyance', 'score': 0.009534290060400963},\n",
       "  {'label': 'pride', 'score': 0.007556593511253595},\n",
       "  {'label': 'amusement', 'score': 0.0075026764534413815},\n",
       "  {'label': 'excitement', 'score': 0.007101245224475861},\n",
       "  {'label': 'optimism', 'score': 0.00500898901373148},\n",
       "  {'label': 'sadness', 'score': 0.004864911548793316},\n",
       "  {'label': 'confusion', 'score': 0.0038028969429433346},\n",
       "  {'label': 'disappointment', 'score': 0.0034991877619177103},\n",
       "  {'label': 'love', 'score': 0.00306585431098938},\n",
       "  {'label': 'anger', 'score': 0.0022725954186171293},\n",
       "  {'label': 'nervousness', 'score': 0.002066229935735464},\n",
       "  {'label': 'embarrassment', 'score': 0.001585035235621035},\n",
       "  {'label': 'desire', 'score': 0.0015397582901641726},\n",
       "  {'label': 'grief', 'score': 0.0015317738289013505},\n",
       "  {'label': 'surprise', 'score': 0.001350597245618701},\n",
       "  {'label': 'remorse', 'score': 0.0013109802966937423},\n",
       "  {'label': 'disgust', 'score': 0.0009403522708453238},\n",
       "  {'label': 'curiosity', 'score': 0.0009379592374898493},\n",
       "  {'label': 'fear', 'score': 0.0008488588500767946}]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_go_emotion_endpoint = \"https://api-inference.huggingface.co/models/SamLowe/roberta-base-go_emotions\"\n",
    "s = \"I am glad that we have no test\"\n",
    "run_query_on_roberta = api_query_runner(roberta_go_emotion_endpoint)\n",
    "data = run_query_on_roberta(s)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider how these informations can be used and apply them. For example, get the overall emotion students have toward a certain course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Explore this model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Bart Large CNN\n",
    "You can read more descriptions here: https://huggingface.co/facebook/bart-large-cnn\n",
    "\n",
    "This model by facebook can summarize a large chunk of texts (there is an upper limit however be aware)\n",
    "How can we utilize this in our analysis? \n",
    "\n",
    "Make API calls like above. HuggingFace does a terrible job of documenting and listing out their endpoints \\(I literally can't find a list). Usually they follow this format \n",
    "- ENDPOINT Template: https://api-inference.huggingface.co/models/<MODEL>. Change the model with different names. E.g: https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english is the endpoint for the distilbert-base-uncased-fintuned-sst-2-english model.\n",
    "\n",
    "However, they do not always follow the pattern, and when that happens, it could be annoying to deal with. One work around I use is to first make a call using their website, and use the browser's developer tool to manually inspect the outgoing request and its endpoint URL.\n",
    "\n",
    "In Chrome: View -> Developer -> Open Developer Tools. All incoming and outgoing requests are in the \"network\" section where they store your communication with different servers.\n",
    "\n",
    "![](https://github.com/MichiganDataScienceTeam/WN2024-RMP/blob/master/notebook/asset/api_workaround.png?raw=true)\n",
    "\n",
    "I have done this already for the Bart Large CNN for your convenience. Apparently for this model we have to also specify /facebook before the model name. \n",
    "Endpoint: https://api-inference.huggingface.co/models/facebook/bart-large-cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Explore the model for summarization tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
